{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import os\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Processing Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_file = \"UD_English-EWT/en_ewt-ud-train.conllu\"\n",
    "lines = open(dataset_file, \"r\").readlines()\n",
    "\n",
    "sentence_word = []\n",
    "sentences_word = []\n",
    "\n",
    "sentence_head_id = []\n",
    "sentences_head_id = []\n",
    "\n",
    "sentence_pos = []\n",
    "sentences_pos = []\n",
    "\n",
    "sentence_deprel = []\n",
    "sentences_deprel = []\n",
    "\n",
    "for line in lines:\n",
    "    l = line.split()\n",
    "    if len(l) > 0 and (l[0].isdigit()):\n",
    "        sentence_word.append(l[1])\n",
    "        sentence_head_id.append(int(l[6]))\n",
    "        sentence_pos.append(l[3])\n",
    "        sentence_deprel.append(l[7])\n",
    "    if len(l) == 0:\n",
    "        sentences_word.append(sentence_word)\n",
    "        sentence_word = []\n",
    "        sentences_head_id.append(sentence_head_id)\n",
    "        sentence_head_id = []\n",
    "        sentences_pos.append(sentence_pos)\n",
    "        sentence_pos = []\n",
    "        sentences_deprel.append(sentence_deprel)\n",
    "        sentence_deprel = []\n",
    "    \n",
    "# sentences_word\n",
    "# sentences_head_id\n",
    "# sentences_pos[0]\n",
    "# sentences_deprel[0]\n",
    "\n",
    "def check_no_child(sta, buff, iden, sentence_h_id):\n",
    "    for i in sta:\n",
    "        if (sentence_h_id[i] == iden):\n",
    "            return False\n",
    "    for i in buff:\n",
    "        if (sentence_h_id[i] == iden):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def rdep(l, ind, tree):\n",
    "    try:\n",
    "        e = l[ind]\n",
    "    except:\n",
    "        return None\n",
    "    for edge in tree:\n",
    "        if edge[0] == e:\n",
    "            return edge[2]\n",
    "    return None\n",
    "\n",
    "def ldep(l, ind, tree):\n",
    "    try:\n",
    "        e = l[ind]\n",
    "    except:\n",
    "        return None\n",
    "    for edge in tree:\n",
    "        if edge[2] == e:\n",
    "            return edge[0]\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote  configurations_training.txt\n"
     ]
    }
   ],
   "source": [
    "filename = \"configurations_training.txt\"\n",
    "\n",
    "try:\n",
    "    os.remove(filename)\n",
    "except OSError:\n",
    "    pass\n",
    "\n",
    "f = open(filename, \"a\")\n",
    "\n",
    "for i in range(len(sentences_word)):\n",
    "#     print(sentences_word[i], sentences_head_id[i])\n",
    "    sentence_word = list(sentences_word[i])\n",
    "    sentence_word.insert(0, 'ROOT')\n",
    "    sentence_head_id = list(sentences_head_id[i])\n",
    "    sentence_head_id.insert(0, -1)\n",
    "    sentence_pos = list(sentences_pos[i])\n",
    "    sentence_pos.insert(0, 'ROOT-POS')\n",
    "    sentence_deprel = list(sentences_deprel[i])\n",
    "    sentence_deprel.insert(0, 'RROOT')\n",
    "    \n",
    "\n",
    "    stack = [0] # 'ROOT' -> 0\n",
    "    buffer = list(range(1, len(sentences_word[i])+1)) # indices of all words\n",
    "    tree = []\n",
    "    \n",
    "    \n",
    "    conf = [\\\n",
    "            sentence_word[stack[-1]] if len(stack)>0 else None, \\\n",
    "            sentence_pos[stack[-1]] if len(stack)>0 else None, \\\n",
    "            sentence_deprel[stack[-1]] if len(stack)>0 else None, \\\n",
    "            sentence_word[buffer[0]] if len(buffer)>0 else None, \\\n",
    "            sentence_pos[buffer[0]] if len(buffer)>0 else None, \\\n",
    "            sentence_deprel[buffer[0]] if len(buffer)>0 else None, \\\n",
    "            sentence_word[buffer[1]] if len(buffer)>1 else None, \\\n",
    "            sentence_pos[buffer[1]] if len(buffer)>1 else None, \\\n",
    "            sentence_deprel[buffer[1]] if len(buffer)>1 else None, \\\n",
    "            sentence_word[ldep(stack, -1, tree)] if ldep(stack, -1, tree)!=None else None, \\\n",
    "            sentence_pos[ldep(stack, -1, tree)] if ldep(stack, -1, tree)!=None else None, \\\n",
    "            sentence_deprel[ldep(stack, -1, tree)] if ldep(stack, -1, tree)!=None else None, \\\n",
    "            sentence_word[rdep(stack, -1, tree)] if rdep(stack, -1, tree)!=None else None, \\\n",
    "            sentence_pos[rdep(stack, -1, tree)] if rdep(stack, -1, tree)!=None else None, \\\n",
    "            sentence_deprel[rdep(stack, -1, tree)] if rdep(stack, -1, tree)!=None else None, \\\n",
    "            sentence_word[ldep(buffer, 0, tree)] if ldep(buffer, 0, tree)!=None else None, \\\n",
    "            sentence_pos[ldep(buffer, 0, tree)] if ldep(buffer, 0, tree)!=None else None, \\\n",
    "            sentence_deprel[ldep(buffer, 0, tree)] if ldep(buffer, 0, tree)!=None else None, \\\n",
    "            sentence_word[rdep(buffer, 0, tree)] if rdep(buffer, 0, tree)!=None else None, \\\n",
    "            sentence_pos[rdep(buffer, 0, tree)] if rdep(buffer, 0, tree)!=None else None, \\\n",
    "            sentence_deprel[rdep(buffer, 0, tree)] if rdep(buffer, 0, tree)!=None else None, \\\n",
    "           ]\n",
    "        \n",
    "    f.write(json.dumps(conf))\n",
    "    \n",
    "    \n",
    "    while len(buffer) > 0:\n",
    "        head_id_stack_right = sentence_head_id[stack[-1]] if len(stack) > 0 else -2\n",
    "        head_id_buffer_left = sentence_head_id[buffer[0]]\n",
    "        \n",
    "        if (head_id_stack_right == buffer[0]):\n",
    "            tree.append((stack[-1], sentence_deprel[stack[-1]], buffer[0])) # add stack[-1] to tree\n",
    "            stack.pop(-1) # left arc\n",
    "            f.write(\" <> 0\\n\")\n",
    "        \n",
    "        elif (len(stack) > 0 and head_id_buffer_left == stack[-1] and \\\n",
    "              check_no_child(stack, buffer, buffer[0], sentence_head_id)):\n",
    "            tree.append((buffer[0], sentence_deprel[buffer[0]], stack[-1])) # add buffer[0] to tree\n",
    "            buffer[0] = stack.pop(-1) # right arc\n",
    "            f.write(\" <> 1\\n\")\n",
    "            \n",
    "        else:\n",
    "            stack.append(buffer.pop(0)) # shift\n",
    "            f.write(\" <> 2\\n\")\n",
    "        \n",
    "        conf = [\\\n",
    "                sentence_word[stack[-1]] if len(stack)>0 else None, \\\n",
    "                sentence_pos[stack[-1]] if len(stack)>0 else None, \\\n",
    "                sentence_deprel[stack[-1]] if len(stack)>0 else None, \\\n",
    "                sentence_word[buffer[0]] if len(buffer)>0 else None, \\\n",
    "                sentence_pos[buffer[0]] if len(buffer)>0 else None, \\\n",
    "                sentence_deprel[buffer[0]] if len(buffer)>0 else None, \\\n",
    "                sentence_word[buffer[1]] if len(buffer)>1 else None, \\\n",
    "                sentence_pos[buffer[1]] if len(buffer)>1 else None, \\\n",
    "                sentence_deprel[buffer[1]] if len(buffer)>1 else None, \\\n",
    "                sentence_word[ldep(stack, -1, tree)] if ldep(stack, -1, tree)!=None else None, \\\n",
    "                sentence_pos[ldep(stack, -1, tree)] if ldep(stack, -1, tree)!=None else None, \\\n",
    "                sentence_deprel[ldep(stack, -1, tree)] if ldep(stack, -1, tree)!=None else None, \\\n",
    "                sentence_word[rdep(stack, -1, tree)] if rdep(stack, -1, tree)!=None else None, \\\n",
    "                sentence_pos[rdep(stack, -1, tree)] if rdep(stack, -1, tree)!=None else None, \\\n",
    "                sentence_deprel[rdep(stack, -1, tree)] if rdep(stack, -1, tree)!=None else None, \\\n",
    "                sentence_word[ldep(buffer, 0, tree)] if ldep(buffer, 0, tree)!=None else None, \\\n",
    "                sentence_pos[ldep(buffer, 0, tree)] if ldep(buffer, 0, tree)!=None else None, \\\n",
    "                sentence_deprel[ldep(buffer, 0, tree)] if ldep(buffer, 0, tree)!=None else None, \\\n",
    "                sentence_word[rdep(buffer, 0, tree)] if rdep(buffer, 0, tree)!=None else None, \\\n",
    "                sentence_pos[rdep(buffer, 0, tree)] if rdep(buffer, 0, tree)!=None else None, \\\n",
    "                sentence_deprel[rdep(buffer, 0, tree)] if rdep(buffer, 0, tree)!=None else None, \\\n",
    "               ]\n",
    "        \n",
    "        f.write(json.dumps(conf))\n",
    "    f.write(\" <> -1\\n\")\n",
    "#     print(i)\n",
    "print(\"Wrote \", filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Processing Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_file = \"UD_English-EWT/en_ewt-ud-test.conllu\"\n",
    "lines = open(dataset_file, \"r\").readlines()\n",
    "\n",
    "sentence_word = []\n",
    "sentences_word = []\n",
    "\n",
    "sentence_head_id = []\n",
    "sentences_head_id = []\n",
    "\n",
    "sentence_pos = []\n",
    "sentences_pos = []\n",
    "\n",
    "sentence_deprel = []\n",
    "sentences_deprel = []\n",
    "\n",
    "for line in lines:\n",
    "    l = line.split()\n",
    "    if len(l) > 0 and (l[0].isdigit()):\n",
    "        sentence_word.append(l[1])\n",
    "        sentence_head_id.append(int(l[6]))\n",
    "        sentence_pos.append(l[3])\n",
    "        sentence_deprel.append(l[7])\n",
    "    if len(l) == 0:\n",
    "        sentences_word.append(sentence_word)\n",
    "        sentence_word = []\n",
    "        sentences_head_id.append(sentence_head_id)\n",
    "        sentence_head_id = []\n",
    "        sentences_pos.append(sentence_pos)\n",
    "        sentence_pos = []\n",
    "        sentences_deprel.append(sentence_deprel)\n",
    "        sentence_deprel = []\n",
    "    \n",
    "# sentences_word\n",
    "# sentences_head_id\n",
    "# sentences_pos[0]\n",
    "# sentences_deprel[0]\n",
    "\n",
    "def check_no_child(sta, buff, iden, sentence_h_id):\n",
    "    for i in sta:\n",
    "        if (sentence_h_id[i] == iden):\n",
    "            return False\n",
    "    for i in buff:\n",
    "        if (sentence_h_id[i] == iden):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def rdep(l, ind, tree):\n",
    "    try:\n",
    "        e = l[ind]\n",
    "    except:\n",
    "        return None\n",
    "    for edge in tree:\n",
    "        if edge[0] == e:\n",
    "            return edge[2]\n",
    "    return None\n",
    "\n",
    "def ldep(l, ind, tree):\n",
    "    try:\n",
    "        e = l[ind]\n",
    "    except:\n",
    "        return None\n",
    "    for edge in tree:\n",
    "        if edge[2] == e:\n",
    "            return edge[0]\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote  configurations_testing.txt\n"
     ]
    }
   ],
   "source": [
    "filename = \"configurations_testing.txt\"\n",
    "\n",
    "try:\n",
    "    os.remove(filename)\n",
    "except OSError:\n",
    "    pass\n",
    "\n",
    "f = open(filename, \"a\")\n",
    "\n",
    "for i in range(len(sentences_word)):\n",
    "#     print(sentences_word[i], sentences_head_id[i])\n",
    "    sentence_word = list(sentences_word[i])\n",
    "    sentence_word.insert(0, 'ROOT')\n",
    "    sentence_head_id = list(sentences_head_id[i])\n",
    "    sentence_head_id.insert(0, -1)\n",
    "    sentence_pos = list(sentences_pos[i])\n",
    "    sentence_pos.insert(0, 'ROOT-POS')\n",
    "    sentence_deprel = list(sentences_deprel[i])\n",
    "    sentence_deprel.insert(0, 'RROOT')\n",
    "    \n",
    "\n",
    "    stack = [0] # 'ROOT' -> 0\n",
    "    buffer = list(range(1, len(sentences_word[i])+1)) # indices of all words\n",
    "    tree = []\n",
    "    \n",
    "    \n",
    "    conf = [\\\n",
    "            sentence_word[stack[-1]] if len(stack)>0 else None, \\\n",
    "            sentence_pos[stack[-1]] if len(stack)>0 else None, \\\n",
    "            sentence_deprel[stack[-1]] if len(stack)>0 else None, \\\n",
    "            sentence_word[buffer[0]] if len(buffer)>0 else None, \\\n",
    "            sentence_pos[buffer[0]] if len(buffer)>0 else None, \\\n",
    "            sentence_deprel[buffer[0]] if len(buffer)>0 else None, \\\n",
    "            sentence_word[buffer[1]] if len(buffer)>1 else None, \\\n",
    "            sentence_pos[buffer[1]] if len(buffer)>1 else None, \\\n",
    "            sentence_deprel[buffer[1]] if len(buffer)>1 else None, \\\n",
    "            sentence_word[ldep(stack, -1, tree)] if ldep(stack, -1, tree)!=None else None, \\\n",
    "            sentence_pos[ldep(stack, -1, tree)] if ldep(stack, -1, tree)!=None else None, \\\n",
    "            sentence_deprel[ldep(stack, -1, tree)] if ldep(stack, -1, tree)!=None else None, \\\n",
    "            sentence_word[rdep(stack, -1, tree)] if rdep(stack, -1, tree)!=None else None, \\\n",
    "            sentence_pos[rdep(stack, -1, tree)] if rdep(stack, -1, tree)!=None else None, \\\n",
    "            sentence_deprel[rdep(stack, -1, tree)] if rdep(stack, -1, tree)!=None else None, \\\n",
    "            sentence_word[ldep(buffer, 0, tree)] if ldep(buffer, 0, tree)!=None else None, \\\n",
    "            sentence_pos[ldep(buffer, 0, tree)] if ldep(buffer, 0, tree)!=None else None, \\\n",
    "            sentence_deprel[ldep(buffer, 0, tree)] if ldep(buffer, 0, tree)!=None else None, \\\n",
    "            sentence_word[rdep(buffer, 0, tree)] if rdep(buffer, 0, tree)!=None else None, \\\n",
    "            sentence_pos[rdep(buffer, 0, tree)] if rdep(buffer, 0, tree)!=None else None, \\\n",
    "            sentence_deprel[rdep(buffer, 0, tree)] if rdep(buffer, 0, tree)!=None else None, \\\n",
    "           ]\n",
    "        \n",
    "    f.write(json.dumps(conf))\n",
    "    \n",
    "    \n",
    "    while len(buffer) > 0:\n",
    "        head_id_stack_right = sentence_head_id[stack[-1]] if len(stack) > 0 else -2\n",
    "        head_id_buffer_left = sentence_head_id[buffer[0]]\n",
    "        \n",
    "        if (head_id_stack_right == buffer[0]):\n",
    "            tree.append((stack[-1], sentence_deprel[stack[-1]], buffer[0])) # add stack[-1] to tree\n",
    "            stack.pop(-1) # left arc\n",
    "            f.write(\" <> 0\\n\")\n",
    "        \n",
    "        elif (len(stack) > 0 and head_id_buffer_left == stack[-1] and \\\n",
    "              check_no_child(stack, buffer, buffer[0], sentence_head_id)):\n",
    "            tree.append((buffer[0], sentence_deprel[buffer[0]], stack[-1])) # add buffer[0] to tree\n",
    "            buffer[0] = stack.pop(-1) # right arc\n",
    "            f.write(\" <> 1\\n\")\n",
    "            \n",
    "        else:\n",
    "            stack.append(buffer.pop(0)) # shift\n",
    "            f.write(\" <> 2\\n\")\n",
    "        \n",
    "        conf = [\\\n",
    "                sentence_word[stack[-1]] if len(stack)>0 else None, \\\n",
    "                sentence_pos[stack[-1]] if len(stack)>0 else None, \\\n",
    "                sentence_deprel[stack[-1]] if len(stack)>0 else None, \\\n",
    "                sentence_word[buffer[0]] if len(buffer)>0 else None, \\\n",
    "                sentence_pos[buffer[0]] if len(buffer)>0 else None, \\\n",
    "                sentence_deprel[buffer[0]] if len(buffer)>0 else None, \\\n",
    "                sentence_word[buffer[1]] if len(buffer)>1 else None, \\\n",
    "                sentence_pos[buffer[1]] if len(buffer)>1 else None, \\\n",
    "                sentence_deprel[buffer[1]] if len(buffer)>1 else None, \\\n",
    "                sentence_word[ldep(stack, -1, tree)] if ldep(stack, -1, tree)!=None else None, \\\n",
    "                sentence_pos[ldep(stack, -1, tree)] if ldep(stack, -1, tree)!=None else None, \\\n",
    "                sentence_deprel[ldep(stack, -1, tree)] if ldep(stack, -1, tree)!=None else None, \\\n",
    "                sentence_word[rdep(stack, -1, tree)] if rdep(stack, -1, tree)!=None else None, \\\n",
    "                sentence_pos[rdep(stack, -1, tree)] if rdep(stack, -1, tree)!=None else None, \\\n",
    "                sentence_deprel[rdep(stack, -1, tree)] if rdep(stack, -1, tree)!=None else None, \\\n",
    "                sentence_word[ldep(buffer, 0, tree)] if ldep(buffer, 0, tree)!=None else None, \\\n",
    "                sentence_pos[ldep(buffer, 0, tree)] if ldep(buffer, 0, tree)!=None else None, \\\n",
    "                sentence_deprel[ldep(buffer, 0, tree)] if ldep(buffer, 0, tree)!=None else None, \\\n",
    "                sentence_word[rdep(buffer, 0, tree)] if rdep(buffer, 0, tree)!=None else None, \\\n",
    "                sentence_pos[rdep(buffer, 0, tree)] if rdep(buffer, 0, tree)!=None else None, \\\n",
    "                sentence_deprel[rdep(buffer, 0, tree)] if rdep(buffer, 0, tree)!=None else None, \\\n",
    "               ]\n",
    "        \n",
    "        f.write(json.dumps(conf))\n",
    "    f.write(\" <> -1\\n\")\n",
    "#     print(i)\n",
    "print(\"Wrote \", filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.scripts import glove2word2vec\n",
    "\n",
    "# glove2word2vec.glove2word2vec('glove.6B/glove.6B.50d.txt', 'glove.6B/glove.6B.50d.txt.word2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format('glove.6B/glove.6B.50d.txt.word2vec', binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load configuration training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n",
      "20000\n"
     ]
    }
   ],
   "source": [
    "filename = \"configurations_training.txt\"\n",
    "f = open(filename, \"r\")\n",
    "lines = f.readlines()\n",
    "\n",
    "embedded_input = []\n",
    "tag_output = []\n",
    "i = 0\n",
    "\n",
    "for line in lines:\n",
    "    i += 1\n",
    "    line = line.split(\"<>\")\n",
    "    c = json.loads(line[0].strip())\n",
    "    t = line[1].strip()\n",
    "#     print(c)\n",
    "    embedding = np.array(0)\n",
    "    for elem in c:\n",
    "        try:\n",
    "            enc = model.wv[elem.lower()]\n",
    "        except:\n",
    "            enc = model.wv['null']\n",
    "        if embedding.size == 1:\n",
    "            embedding = np.append(embedding, enc)\n",
    "            embedding = np.delete(embedding, 0)\n",
    "        else:\n",
    "            embedding = np.append(embedding, enc)\n",
    "            \n",
    "    embedded_input.append(embedding)\n",
    "    tag_output.append(int(t))\n",
    "    \n",
    "    if i == 20000:\n",
    "        break\n",
    "\n",
    "print(len(embedded_input))\n",
    "print(len(tag_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPClassifier(solver='adam', alpha=1e-5, hidden_layer_sizes=(50, 10), random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(50, 10), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
       "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "       warm_start=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(embedded_input, tag_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction = clf.predict(embedded_input).tolist()\n",
    "# actual_tag = tag_output\n",
    "# # print(prediction)\n",
    "# # print(actual_tag)\n",
    "\n",
    "# c = 0\n",
    "# for i in range(len(prediction)):\n",
    "#     if prediction[i] == actual_tag[i]:\n",
    "#         c += 1\n",
    "# print(c*100/len(prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load configuration test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000\n",
      "8000\n"
     ]
    }
   ],
   "source": [
    "filename = \"configurations_testing.txt\"\n",
    "f = open(filename, \"r\")\n",
    "lines = f.readlines()\n",
    "\n",
    "test_input = []\n",
    "test_output = []\n",
    "i = 0\n",
    "\n",
    "for line in lines:\n",
    "    i += 1\n",
    "    line = line.split(\"<>\")\n",
    "    c = json.loads(line[0].strip())\n",
    "    t = line[1].strip()\n",
    "#     print(c)\n",
    "    embedding = np.array(0)\n",
    "    for elem in c:\n",
    "        try:\n",
    "            enc = model.wv[elem.lower()]\n",
    "        except:\n",
    "            enc = model.wv['null']\n",
    "        if embedding.size == 1:\n",
    "            embedding = np.append(embedding, enc)\n",
    "            embedding = np.delete(embedding, 0)\n",
    "        else:\n",
    "            embedding = np.append(embedding, enc)\n",
    "            \n",
    "    test_input.append(embedding)\n",
    "    test_output.append(int(t))\n",
    "    \n",
    "    if i == 8000:\n",
    "        break\n",
    "\n",
    "print(len(test_input))\n",
    "print(len(test_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  90.4125\n"
     ]
    }
   ],
   "source": [
    "prediction = clf.predict(test_input).tolist()\n",
    "actual_tag = test_output\n",
    "# print(prediction)\n",
    "# print(actual_tag)\n",
    "\n",
    "c = 0\n",
    "for i in range(len(prediction)):\n",
    "    if prediction[i] == actual_tag[i]:\n",
    "        c += 1\n",
    "print(\"Accuracy: \", c*100/len(prediction))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
